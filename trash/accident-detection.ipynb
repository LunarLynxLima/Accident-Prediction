{"cells":[{"cell_type":"markdown","metadata":{"papermill":{"duration":0.012607,"end_time":"2024-05-15T05:45:29.670720","exception":false,"start_time":"2024-05-15T05:45:29.658113","status":"completed"},"tags":[]},"source":["<h1> Accident Detection From CCTV Footage </h1>"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.011721,"end_time":"2024-05-15T05:45:29.694146","exception":false,"start_time":"2024-05-15T05:45:29.682425","status":"completed"},"tags":[]},"source":["\n","## Overview\n","### Baseline paper :\n","Large Multi-Modal Foundation Model for Traffic Accident Analysis [https://arxiv.org/pdf/2401.03040 ]\n","‚Å†LLM Multimodal Traffic Accident Forecasting [ https://www.mdpi.com/1424-8220/23/22/9225 ]\n","\n","## Brief Overview:\n","Multi-Modal Traffic Accident Analysis for Safer Roads:- Develop an innovative model to analyze diverse traffic data, uncover accident root causes, and proactively suggest preventive solutions.\n","\n","## Description\n","### The Challenge\n","Traffic accidents remain a persistent global threat despite extensive safety efforts.\n","Traditional models often centre on single data sources, failing to capture the complex interplay of factors contributing to accidents.\n","A holistic, multi-modal approach is needed to understand and mitigate traffic risks effectively.\n","\n","## The Task\n","Construct a model that seamlessly integrates and analyzes data from various sources:\n","\n","1. Vehicular data (speed, GPS, sensor readings)\n","2. Pedestrian behavior (movement patterns, crossings)\n","3. CCTV footage (traffic flow, potential incidents)\n","4. Weather conditions (visibility, precipitation)\n","5. Road infrastructure (layout, signage, condition)\n","\n","The model's insights should pinpoint the leading causes of accidents and inform potential preventive measures.\n","\n","Given a dataset containing multimodal information such as images, videos, and textual descriptions of road scenes, the goal is to develop a robust accident detection system using a Multimodal Language Model (LLM). The system should accurately classify each scene into one of two categories: \"no accident\" (label 0) or \"accident\" (label 1).\n","\n","\n","## Scoring\n","Data preparation - 20% <br>\n","Evaluation - 20 % <br>\n","Plots - 20 % <br>\n","Model finetuning - 40% <br>"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.011443,"end_time":"2024-05-15T05:45:29.717104","exception":false,"start_time":"2024-05-15T05:45:29.705661","status":"completed"},"tags":[]},"source":["<h1>1. Loading Data</h1>"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:57:51.338846Z"},"papermill":{"duration":11.656721,"end_time":"2024-05-15T05:45:41.421953","exception":false,"start_time":"2024-05-15T05:45:29.765232","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":14,"metadata":{"papermill":{"duration":2.514304,"end_time":"2024-05-15T05:45:43.948147","exception":false,"start_time":"2024-05-15T05:45:41.433843","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 791 files belonging to 2 classes.\n"]}],"source":["training_data_dir = os.path.join(\"/kaggle/input/accident-detection-from-cctv-footage/data/train\")\n","training_data_dir = os.path.join(\"data/train\")\n","training_data = tf.keras.utils.image_dataset_from_directory(\n","                            training_data_dir,image_size=(256, 256),\n","                            seed = 42\n","                            )"]},{"cell_type":"code","execution_count":15,"id":"9a8bc5d1","metadata":{"papermill":{"duration":1.101441,"end_time":"2024-05-15T05:45:45.061537","exception":false,"start_time":"2024-05-15T05:45:43.960096","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["#  iterator extracts in each batch of 32 images \n","training_data_iterator = training_data.as_numpy_iterator()\n","training_batch = training_data_iterator.next()"]},{"cell_type":"markdown","id":"4a7b28fe","metadata":{"papermill":{"duration":0.049282,"end_time":"2024-05-15T05:45:49.779984","exception":false,"start_time":"2024-05-15T05:45:49.730702","status":"completed"},"tags":[]},"source":["<h1>2. Preprocessing Data </h1>"]},{"cell_type":"code","execution_count":16,"id":"4495585c","metadata":{"papermill":{"duration":0.081079,"end_time":"2024-05-15T05:45:49.911488","exception":false,"start_time":"2024-05-15T05:45:49.830409","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Max pixel value :  1.0\n","Min pixel value :  0.0\n"]}],"source":["# Normalizing rgb pixels value between between 0 & 1 \n","training_data = training_data.map(lambda x,y: (x/255, y))\n","training_batch = training_data.as_numpy_iterator().next()\n","\n","# Sanity Check pixel min/max pixel values after normalization\n","print(\"Max pixel value : \",training_batch[0].max())\n","print(\"Min pixel value : \",training_batch[0].min())"]},{"cell_type":"markdown","id":"11fb50e7","metadata":{"papermill":{"duration":0.089998,"end_time":"2024-05-15T05:45:55.536259","exception":false,"start_time":"2024-05-15T05:45:55.446261","status":"completed"},"tags":[]},"source":["<h2>Loading Validation data for Hyper-parameter Turing</h2>"]},{"cell_type":"code","execution_count":17,"id":"0faba709","metadata":{"papermill":{"duration":0.515378,"end_time":"2024-05-15T05:45:56.137784","exception":false,"start_time":"2024-05-15T05:45:55.622406","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 98 files belonging to 2 classes.\n"]}],"source":["validation_data_dir = os.path.join(\"/kaggle/input/accident-detection-from-cctv-footage/data/val\")\n","validation_data_dir = os.path.join(\"data/val\")\n","\n","validation_data = tf.keras.utils.image_dataset_from_directory(validation_data_dir)\n","validation_data_iterator = validation_data.as_numpy_iterator()\n","validation_batch = validation_data_iterator.next()"]},{"cell_type":"code","execution_count":18,"id":"c8614a28","metadata":{"papermill":{"duration":0.365998,"end_time":"2024-05-15T05:45:56.594839","exception":false,"start_time":"2024-05-15T05:45:56.228841","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Max pixel value :  1.0\n","Min pixel value :  0.0\n"]}],"source":["# Normalizing Validation data\n","validation_data = validation_data.map(lambda x,y: (x/255, y))\n","validation_batch = validation_data.as_numpy_iterator().next()\n","\n","# Sanity Check pixel min/max pixel values after normalization\n","print(\"Max pixel value : \",validation_batch[0].max())\n","print(\"Min pixel value : \",validation_batch[0].min())"]},{"cell_type":"markdown","id":"70a03ea0","metadata":{"papermill":{"duration":0.085089,"end_time":"2024-05-15T05:45:56.948892","exception":false,"start_time":"2024-05-15T05:45:56.863803","status":"completed"},"tags":[]},"source":["<h1> 3. Building CNN Architecture  </h1>\n"]},{"cell_type":"code","execution_count":19,"id":"6ec1a0fa","metadata":{"papermill":{"duration":0.097538,"end_time":"2024-05-15T05:45:57.131534","exception":false,"start_time":"2024-05-15T05:45:57.033996","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Add, Dropout\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.optimizers import Adam"]},{"cell_type":"code","execution_count":26,"id":"a77d30cf","metadata":{},"outputs":[{"ename":"ImportError","evalue":"\nAutoModel requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModel\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Create an instance of the custom model\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCustomModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[26], line 11\u001b[0m, in \u001b[0;36mCustomModel.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28msuper\u001b[39m(CustomModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load the transformer-based model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlpconnect/vit-gpt2-image-captioning\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Freeze the transformer-based model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_model\u001b[38;5;241m.\u001b[39mparameters():\n","File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1450\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1450\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1429\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m is_tf_available():\n\u001b[0;32m-> 1429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n","\u001b[0;31mImportError\u001b[0m: \nAutoModel requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModel\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from transformers import AutoModel\n","\n","class CustomModel(nn.Module):\n","    def __init__(self):\n","        super(CustomModel, self).__init__()\n","        # Load the transformer-based model\n","        self.text_model = AutoModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n","        \n","        # Freeze the transformer-based model\n","        for param in self.text_model.parameters():\n","            param.requires_grad = False\n","        \n","        # CNN layers for image processing\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n","        self.conv3 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n","        \n","        # Fully connected layers for final classification\n","        self.fc1 = nn.Linear(16 * 64 * 64 + 768, 128)  # Concatenate image and text embeddings\n","        self.dropout = nn.Dropout(0.5)\n","        self.fc2 = nn.Linear(128, 1)\n","\n","    def forward(self, images, text_input):\n","        # Process image through CNN layers\n","        x = self.pool(F.relu(self.conv1(images)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.pool(F.relu(self.conv3(x)))\n","        x = x.view(-1, 16 * 64 * 64)  # Adjust the size here\n","        \n","        # Process text through transformer-based model\n","        text_outputs = self.text_model(**text_input).last_hidden_state[:, 0, :]  # Use the CLS token\n","        \n","        # Concatenate image and text embeddings\n","        concatenated_features = torch.cat((x, text_outputs), dim=1)\n","        \n","        # Fully connected layers for final classification\n","        x = F.relu(self.fc1(concatenated_features))\n","        x = self.dropout(x)\n","        x = torch.sigmoid(self.fc2(x))\n","        \n","        return x\n","\n","# Create an instance of the custom model\n","model = CustomModel()\n","for name, param in model.named_parameters():\n","    print(f\"Layer: {name} | Size: {param.size()} | Values: {param}\")\n"]},{"cell_type":"code","execution_count":27,"id":"d37db545","metadata":{},"outputs":[{"ename":"ImportError","evalue":"\nAutoModel requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModel\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Create an instance of the custom model\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCustomModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[27], line 12\u001b[0m, in \u001b[0;36mCustomModel.__init__\u001b[0;34m(self, image_embedding_dim)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28msuper\u001b[39m(CustomModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load the pre-trained image-to-text model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_to_text_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlpconnect/vit-gpt2-image-captioning\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Freeze the image-to-text model\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_to_text_model\u001b[38;5;241m.\u001b[39mparameters():\n","File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1450\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1450\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1429\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m is_tf_available():\n\u001b[0;32m-> 1429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n","\u001b[0;31mImportError\u001b[0m: \nAutoModel requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModel\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from transformers import AutoModel\n","\n","class CustomModel(nn.Module):\n","    def __init__(self, image_embedding_dim=768):\n","        super(CustomModel, self).__init__()\n","        \n","        # Load the pre-trained image-to-text model\n","        self.img_to_text_model = AutoModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n","        \n","        # Freeze the image-to-text model\n","        for param in self.img_to_text_model.parameters():\n","            param.requires_grad = False\n","        \n","        # Define the CNN architecture for image feature extraction\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n","        \n","        # Define the fully connected layer\n","        self.fc1 = nn.Linear(in_features=32 * 64 * 64 + image_embedding_dim, out_features=128)\n","        self.fc2 = nn.Linear(in_features=128, out_features=1)\n","        \n","    def forward(self, images, text_input):\n","        # Process images through CNN\n","        x = self.pool(F.relu(self.conv1(images)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 32 * 64 * 64)  # Flatten\n","        \n","        # Process text through the image-to-text model\n","        text_outputs = self.img_to_text_model(**text_input).last_hidden_state[:, 0, :]  # Use the CLS token\n","        \n","        # Concatenate image and text embeddings\n","        x = torch.cat((x, text_outputs), dim=1)\n","        \n","        # Fully connected layers\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        x = torch.sigmoid(x)\n","        \n","        return x\n","\n","# Create an instance of the custom model\n","model = CustomModel()\n"]},{"cell_type":"code","execution_count":null,"id":"66abf294","metadata":{"papermill":{"duration":0.100246,"end_time":"2024-05-15T05:45:57.358321","exception":false,"start_time":"2024-05-15T05:45:57.258075","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# # Define input layer\n","# inputs = Input(shape=(256, 256, 3))\n","\n","# # First Convolutional Block\n","# x = Conv2D(16, (3,3), 1, activation='relu', padding='same')(inputs)\n","# x = MaxPooling2D()(x)\n","\n","# # Second Convolutional Block with residual connection\n","# conv1 = Conv2D(32, (3,3), 1, activation='relu', padding='same')(x)\n","# conv2 = Conv2D(32, (3,3), 1, activation='relu', padding='same')(conv1)\n","# # Adding convolutional layer to match the number of channels\n","# residual = Conv2D(32, (1, 1), strides=(1, 1), padding='same')(x)\n","# residual = Add()([residual, conv2])\n","# x = MaxPooling2D()(residual)\n","\n","# # Third Convolutional Block with residual connection\n","# conv3 = Conv2D(16, (3,3), 1, activation='relu', padding='same')(x)\n","# conv4 = Conv2D(16, (3,3), 1, activation='relu', padding='same')(conv3)\n","# # Adding convolutional layer to match the number of channels\n","# residual = Conv2D(16, (1, 1), strides=(1, 1), padding='same')(x)\n","# residual = Add()([residual, conv4])\n","# x = MaxPooling2D()(residual)\n","\n","# # Flatten layer\n","# x = Flatten()(x)\n","\n","# # Fully connected layers\n","# x = Dense(256, activation='relu')(x)\n","# outputs = Dense(1, activation='sigmoid')(x)\n","\n","# model = Model(inputs = inputs, outputs=outputs)"]},{"cell_type":"code","execution_count":null,"id":"55ad036e","metadata":{"papermill":{"duration":0.226306,"end_time":"2024-05-15T05:45:57.672649","exception":false,"start_time":"2024-05-15T05:45:57.446343","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# model = Sequential()\n","\n","# model.add(Conv2D(16, (3,3), 1, activation='relu', input_shape=(256,256,3)))\n","# model.add(MaxPooling2D())\n","# model.add(Conv2D(32, (3,3), 1, activation='relu'))\n","# model.add(MaxPooling2D())\n","# model.add(Conv2D(16, (3,3), 1, activation='relu'))\n","# model.add(MaxPooling2D())\n","# model.add(Flatten())\n","# # Adding neural Layer\n","# model.add(Dense(256, activation='relu'))\n","# model.add(Dense(1, activation='sigmoid'))"]},{"cell_type":"code","execution_count":null,"id":"f825f06d","metadata":{"papermill":{"duration":0.116045,"end_time":"2024-05-15T05:45:57.891002","exception":false,"start_time":"2024-05-15T05:45:57.774957","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["learning_rate = 0.001 \n","optimizer = Adam(learning_rate=learning_rate)\n","model.compile(optimizer = optimizer, loss='binary_crossentropy', metrics = ['accuracy'])"]},{"cell_type":"code","execution_count":null,"id":"3eb8274e","metadata":{"papermill":{"duration":0.111979,"end_time":"2024-05-15T05:45:58.099358","exception":false,"start_time":"2024-05-15T05:45:57.987379","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","id":"a309f66c","metadata":{"papermill":{"duration":0.095997,"end_time":"2024-05-15T05:45:58.293959","exception":false,"start_time":"2024-05-15T05:45:58.197962","status":"completed"},"tags":[]},"source":["<h1> 4.  Training Convolutional Neural Network </h1>"]},{"cell_type":"code","execution_count":null,"id":"6cbd58c2","metadata":{"papermill":{"duration":0.103221,"end_time":"2024-05-15T05:45:58.490141","exception":false,"start_time":"2024-05-15T05:45:58.386920","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# setting up for logging \n","logdir='logs'\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n","early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"]},{"cell_type":"code","execution_count":null,"id":"df9c7989","metadata":{"papermill":{"duration":65.339925,"end_time":"2024-05-15T05:47:03.924388","exception":false,"start_time":"2024-05-15T05:45:58.584463","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["bst_model = model.fit(training_data, epochs=50, validation_data=validation_data, callbacks=[early_stopping_callback])\n","bst_model = model.fit(training_data, epochs=50, validation_data=validation_data, callbacks=[tensorboard_callback])\n","model.save(\"/kaggle/working/accidents.keras\")                                                     "]},{"cell_type":"code","execution_count":null,"id":"556b6297","metadata":{"papermill":{"duration":0.135552,"end_time":"2024-05-15T05:47:04.194050","exception":false,"start_time":"2024-05-15T05:47:04.058498","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["bst_model.history['validation_accuracy'][-1]"]},{"cell_type":"markdown","id":"1563766f","metadata":{"papermill":{"duration":0.129722,"end_time":"2024-05-15T05:47:04.449212","exception":false,"start_time":"2024-05-15T05:47:04.319490","status":"completed"},"tags":[]},"source":["<h2>5. Plotting Training Loss and Accuracy Curve with epochs</h2>"]},{"cell_type":"code","execution_count":null,"id":"941e8550","metadata":{"papermill":{"duration":0.439505,"end_time":"2024-05-15T05:47:05.028922","exception":false,"start_time":"2024-05-15T05:47:04.589417","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["fig = plt.figure()\n","plt.plot(bst_model.history['loss'], color='red', label='training loss')\n","plt.plot(bst_model.history['validation_loss'], color='blue', label='validation_loss')\n","fig.suptitle('Loss', fontsize=20)\n","plt.legend(loc=\"upper left\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"loss\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"5834fd80","metadata":{"papermill":{"duration":0.406352,"end_time":"2024-05-15T05:47:05.561621","exception":false,"start_time":"2024-05-15T05:47:05.155269","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["fig = plt.figure()\n","plt.plot(bst_model.history['accuracy'], color='red', label='training accuracy')\n","plt.plot(bst_model.history['validation_accuracy'], color='blue', label='validation_accuracy')\n","fig.suptitle('Accuracy', fontsize=20)\n","plt.legend(loc=\"upper left\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.show()"]},{"cell_type":"markdown","id":"4465f085","metadata":{"papermill":{"duration":0.127763,"end_time":"2024-05-15T05:47:05.825994","exception":false,"start_time":"2024-05-15T05:47:05.698231","status":"completed"},"tags":[]},"source":["# 6. Evaluation\n","Model performance will be measured by its F1 scores in predicting and analyzing actual traffic accidents.\n","Solutions offering actionable insights and demonstrable potential to reduce accident frequency and impact will be favoured."]},{"cell_type":"code","execution_count":null,"id":"34ddb8fb","metadata":{"papermill":{"duration":0.591539,"end_time":"2024-05-15T05:47:06.545518","exception":false,"start_time":"2024-05-15T05:47:05.953979","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["test_data_dir = os.path.join(\"/kaggle/input/accident-detection-from-cctv-footage/data/test\")\n","test_data = tf.keras.utils.image_dataset_from_directory(test_data_dir)\n","test_data_iterator = test_data.as_numpy_iterator()\n","test_batch = test_data_iterator.next()"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":2.006276,"end_time":"2024-05-15T05:47:08.944765","exception":false,"start_time":"2024-05-15T05:47:06.938489","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, classification_report, f1_score, recall_score, precision_score\n","\n","y_true = []  # true labels\n","y_pred = []  # predicted labels\n","\n","for batch in test_data:\n","    X, y = batch\n","    yhat = model.predict(X)\n","    y_pred.extend(yhat.flatten().round().astype(int))\n","    y_true.extend(y.flatten().astype(int))\n","    \n","# Calculate evaluation metrics\n","f1 = f1_score(y_true, y_pred)\n","precision = precision_score(y_true, y_pred)\n","recall = recall_score(y_true, y_pred)\n","cm = confusion_matrix(y_true, y_pred)\n","\n","# Print evaluation metrics and confusion matrix\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1 score:\", f1)\n","print(\"Confusion Matrix:\")\n","print(cm)\n","\n","# Generate classification report\n","target_names = ['Not Accident', 'Accident']\n","classification_rep = classification_report(y_true, y_pred, target_names=target_names)\n","print(\"Classification Report:\")\n","print(classification_rep)"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.15238,"end_time":"2024-05-15T05:47:09.528727","exception":false,"start_time":"2024-05-15T05:47:09.376347","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def F1_score(precision, recall):\n","    return (2*precision*recall)/(precision+recall)\n","\n","pre = tf.keras.metrics.Precision()\n","re = tf.keras.metrics.Recall()\n","\n","for batch in test_data:\n","    X, y = batch\n","    yhat = model.predict(X)\n","    pre.update_state(y, yhat)\n","    re.update_state(y, yhat)\n","\n","print(\"Model achieved an precision score of {:5f}\".format(pre.result()))\n","print(\"Model achieved an recall score of {:5f}\".format(re.result()))\n","\n","f1_score = F1_score(pre.result(), re.result())\n","print(\"Model achieved an F1-score of {:5f}\".format(f1_score))"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.129295,"end_time":"2024-05-15T05:47:10.062212","exception":false,"start_time":"2024-05-15T05:47:09.932917","status":"completed"},"tags":[]},"source":["<h1> 7.Test just to see model working </h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":1.434368,"end_time":"2024-05-15T05:47:11.627265","exception":false,"start_time":"2024-05-15T05:47:10.192897","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import cv2\n","\n","# load random samples from samples directory\n","random_data_dirname = os.path.join(\"/kaggle/input/accident-detection-from-cctv-footage/data/test/Accident\")\n","pics = [os.path.join(random_data_dirname, filename) for filename in os.listdir(random_data_dirname)]\n","\n","# load first file from samples\n","sample = cv2.imread(pics[1], cv2.IMREAD_COLOR)\n","sample = cv2.resize(sample, (256, 256))\n","\n","prediction = 1 - model.predict(np.expand_dims(sample/255, 0))\n","\n","if prediction >= 0.5: \n","    label = 'Predicted class is Accident'\n","else:\n","    label = 'Predicted class is Not Accident'\n","\n","plt.title(label)\n","plt.imshow(sample)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.136074,"end_time":"2024-05-15T05:47:11.903298","exception":false,"start_time":"2024-05-15T05:47:11.767224","status":"completed"},"tags":[]},"source":["### Create CSV Files for Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":7.355022,"end_time":"2024-05-15T05:47:19.391935","exception":false,"start_time":"2024-05-15T05:47:12.036913","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import cv2\n","import pandas as pd\n","\n","# load random samples from samples directory\n","test_data_dirname = os.path.join(\"/kaggle/input/accident-detection-from-cctv-footage/data/test\")\n","pics = [os.path.join(test_data_dirname, filename) for filename in os.listdir(test_data_dirname)]\n","\n","\n","filenames = []\n","predictions = []\n","\n","for dirname in os.listdir(test_data_dirname):\n","    for filename in os.listdir(os.path.join(test_data_dirname, dirname)):\n","        if not filename.endswith(\".jpg\"):\n","            continue\n","        filepath = os.path.join(test_data_dirname, dirname, filename)\n","        \n","        # load first file from samples\n","        sample = cv2.imread(filepath, cv2.IMREAD_COLOR)\n","        sample = cv2.resize(sample, (256, 256))\n","        \n","        # predict using model\n","        prediction = 1 - model.predict(np.expand_dims(sample/255, 0))\n","        # done because when we loaded data by default 0 label is given to first folder\n","        # which is Accident but we want just opposite labels\n","        # 0: Accident, 1: Not Accident\n","        \n","        filenames.append(filename)\n","        \n","        output = 1 if float(prediction[0][0]) >= 0.5 else 0\n","        predictions.append(output)\n","\n","df = pd.DataFrame(columns=[\"ID\", \"Column ID\"])\n","df[\"ID\"] = filenames\n","df[\"Column ID\"] = predictions\n","df.to_csv(\"/kaggle/working/submission.csv\",index=False)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8428710,"sourceId":77305,"sourceType":"competition"},{"datasetId":804753,"sourceId":1379553,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"papermill":{"default_parameters":{},"duration":116.215069,"end_time":"2024-05-15T05:47:23.279419","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-15T05:45:27.064350","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}
