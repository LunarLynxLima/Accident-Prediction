{"cells":[{"cell_type":"markdown","metadata":{"papermill":{"duration":0.012607,"end_time":"2024-05-15T05:45:29.670720","exception":false,"start_time":"2024-05-15T05:45:29.658113","status":"completed"},"tags":[]},"source":["<h1> Accident Detection From CCTV Footage </h1>"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.011721,"end_time":"2024-05-15T05:45:29.694146","exception":false,"start_time":"2024-05-15T05:45:29.682425","status":"completed"},"tags":[]},"source":["\n","## 0. Overview\n","### Baseline paper :\n","Large Multi-Modal Foundation Model for Traffic Accident Analysis [https://arxiv.org/pdf/2401.03040 ]\n","‚Å†LLM Multimodal Traffic Accident Forecasting [ https://www.mdpi.com/1424-8220/23/22/9225 ]\n","\n","## Brief Overview:\n","Multi-Modal Traffic Accident Analysis for Safer Roads:- Develop an innovative model to analyze diverse traffic data, uncover accident root causes, and proactively suggest preventive solutions.\n","\n","## Description\n","### The Challenge\n","Traffic accidents remain a persistent global threat despite extensive safety efforts.\n","Traditional models often centre on single data sources, failing to capture the complex interplay of factors contributing to accidents.\n","A holistic, multi-modal approach is needed to understand and mitigate traffic risks effectively.\n","\n","## The Task\n","Construct a model that seamlessly integrates and analyzes data from various sources:\n","\n","1. Vehicular data (speed, GPS, sensor readings)\n","2. Pedestrian behavior (movement patterns, crossings)\n","3. CCTV footage (traffic flow, potential incidents)\n","4. Weather conditions (visibility, precipitation)\n","5. Road infrastructure (layout, signage, condition)\n","\n","The model's insights should pinpoint the leading causes of accidents and inform potential preventive measures.\n","\n","Given a dataset containing multimodal information such as images, videos, and textual descriptions of road scenes, the goal is to develop a robust accident detection system using a Multimodal Language Model (LLM). The system should accurately classify each scene into one of two categories: \"no accident\" (label 0) or \"accident\" (label 1).\n","\n","\n","## Scoring\n","Data preparation - 20% <br>\n","Evaluation - 20 % <br>\n","Plots - 20 % <br>\n","Model finetuning - 40% <br>"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.011443,"end_time":"2024-05-15T05:45:29.717104","exception":false,"start_time":"2024-05-15T05:45:29.705661","status":"completed"},"tags":[]},"source":["<h1>1. Loading Data</h1>"]},{"cell_type":"code","execution_count":210,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T00:11:08.958057Z","iopub.status.busy":"2024-05-16T00:11:08.957407Z","iopub.status.idle":"2024-05-16T00:11:08.962892Z","shell.execute_reply":"2024-05-16T00:11:08.961945Z","shell.execute_reply.started":"2024-05-16T00:11:08.958025Z"},"papermill":{"duration":11.656721,"end_time":"2024-05-15T05:45:41.421953","exception":false,"start_time":"2024-05-15T05:45:29.765232","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":211,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T00:11:08.966241Z","iopub.status.busy":"2024-05-16T00:11:08.965932Z","iopub.status.idle":"2024-05-16T00:11:09.275821Z","shell.execute_reply":"2024-05-16T00:11:09.274919Z","shell.execute_reply.started":"2024-05-16T00:11:08.966208Z"},"papermill":{"duration":2.514304,"end_time":"2024-05-15T05:45:43.948147","exception":false,"start_time":"2024-05-15T05:45:41.433843","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 791 files belonging to 2 classes.\n"]}],"source":["training_data_dir = os.path.join(\"/kaggle/input/accident-detection-from-cctv-footage/data/train\")\n","training_data = tf.keras.utils.image_dataset_from_directory(\n","                            training_data_dir,image_size=(256, 256),\n","                            seed = 42\n","                            )"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":1.101441,"end_time":"2024-05-15T05:45:45.061537","exception":false,"start_time":"2024-05-15T05:45:43.960096","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["#  iterator extracts in each batch of 32 images \n","training_data_iterator = training_data.as_numpy_iterator()\n","training_batch = training_data_iterator.next()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.049282,"end_time":"2024-05-15T05:45:49.779984","exception":false,"start_time":"2024-05-15T05:45:49.730702","status":"completed"},"tags":[]},"source":["<h1>2. Preprocessing Data </h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.081079,"end_time":"2024-05-15T05:45:49.911488","exception":false,"start_time":"2024-05-15T05:45:49.830409","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Normalizing rgb pixels value between between 0 & 1 \n","training_data = training_data.map(lambda x,y: (x/255, y))\n","training_batch = training_data.as_numpy_iterator().next()\n","\n","# Sanity Check pixel min/max pixel values after normalization\n","print(\"Max pixel value : \",training_batch[0].max())\n","print(\"Min pixel value : \",training_batch[0].min())"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.089998,"end_time":"2024-05-15T05:45:55.536259","exception":false,"start_time":"2024-05-15T05:45:55.446261","status":"completed"},"tags":[]},"source":["<h2>Loading Validation data for Hyper-parameter Turing</h2>"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.515378,"end_time":"2024-05-15T05:45:56.137784","exception":false,"start_time":"2024-05-15T05:45:55.622406","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["validation_data_dir = os.path.join(\"/kaggle/input/accident-detection-from-cctv-footage/data/val\")\n","validation_data = tf.keras.utils.image_dataset_from_directory(validation_data_dir)\n","validation_data_iterator = validation_data.as_numpy_iterator()\n","validation_batch = validation_data_iterator.next()"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.365998,"end_time":"2024-05-15T05:45:56.594839","exception":false,"start_time":"2024-05-15T05:45:56.228841","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Normalizing Validation data\n","validation_data = validation_data.map(lambda x,y: (x/255, y))\n","validation_batch = validation_data.as_numpy_iterator().next()\n","\n","# Sanity Check pixel min/max pixel values after normalization\n","print(\"Max pixel value : \",validation_batch[0].max())\n","print(\"Min pixel value : \",validation_batch[0].min())"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.085089,"end_time":"2024-05-15T05:45:56.948892","exception":false,"start_time":"2024-05-15T05:45:56.863803","status":"completed"},"tags":[]},"source":["<h1> 3. Building CNN Architecture  </h1>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.097538,"end_time":"2024-05-15T05:45:57.131534","exception":false,"start_time":"2024-05-15T05:45:57.033996","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Add, Dropout\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.optimizers import Adam"]},{"cell_type":"markdown","metadata":{},"source":["#### Arch-1\n","Deep CNN With residual connections followed by classifying dense layer"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.100246,"end_time":"2024-05-15T05:45:57.358321","exception":false,"start_time":"2024-05-15T05:45:57.258075","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# # Define input layer\n","# inputs = Input(shape=(256, 256, 3))\n","\n","# # First Convolutional Block\n","# x = Conv2D(16, (3,3), 1, activation='relu', padding='same')(inputs)\n","# x = MaxPooling2D()(x)\n","\n","# # Second Convolutional Block with residual connection\n","# conv1 = Conv2D(32, (3,3), 1, activation='relu', padding='same')(x)\n","# conv2 = Conv2D(32, (3,3), 1, activation='relu', padding='same')(conv1)\n","# # Adding convolutional layer to match the number of channels\n","# residual = Conv2D(32, (1, 1), strides=(1, 1), padding='same')(x)\n","# residual = Add()([residual, conv2])\n","# x = MaxPooling2D()(residual)\n","\n","# # Third Convolutional Block with residual connection\n","# conv3 = Conv2D(16, (3,3), 1, activation='relu', padding='same')(x)\n","# conv4 = Conv2D(16, (3,3), 1, activation='relu', padding='same')(conv3)\n","# # Adding convolutional layer to match the number of channels\n","# residual = Conv2D(16, (1, 1), strides=(1, 1), padding='same')(x)\n","# residual = Add()([residual, conv4])\n","# x = MaxPooling2D()(residual)\n","\n","# # Add another Convolutional Layer\n","# x = Conv2D(8, (3,3), 1, activation='relu', padding='same')(x)\n","# x = MaxPooling2D()(x)\n","\n","# # Flatten layer\n","# x = Flatten()(x)\n","\n","# # Fully connected layers\n","# x = Dense(32, activation='relu')(x)\n","# x = Dense(16, activation='relu')(x)\n","# x = Dense(8, activation='relu')(x)\n","# outputs = Dense(1, activation='sigmoid')(x)\n","\n","# model = Model(inputs = inputs, outputs=outputs)"]},{"cell_type":"markdown","metadata":{},"source":["## Load older model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# from tensorflow.keras.models import load_model\n","\n","# # Provide the path to the saved model\n","# # model_path = \"/kaggle/working/accidents.keras\"\n","# model_path = \"model.keras\"\n","\n","\n","# # Load the model\n","# loaded_model = load_model(model_path)\n","# model = loaded_model"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.226306,"end_time":"2024-05-15T05:45:57.672649","exception":false,"start_time":"2024-05-15T05:45:57.446343","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["model = Sequential()\n","\n","model.add(Conv2D(16, (3,3), 1, activation='relu', input_shape=(256,256,3)))\n","model.add(MaxPooling2D())\n","\n","model.add(Conv2D(32, (3,3), 1, activation='relu'))\n","model.add(MaxPooling2D())\n","\n","model.add(Conv2D(16, (3,3), 1, activation='relu'))\n","model.add(MaxPooling2D())\n","\n","model.add(Flatten())\n","\n","\n","# Adding neural Layer\n","model.add(Dense(256, activation='relu'))\n","# model.add(Dense(64, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))"]},{"cell_type":"markdown","metadata":{},"source":["## Arch - 2\n","\n","Using residual connections in cnn \n","And deeper dense layers"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n","# from tensorflow.keras.models import Model\n","\n","# # Define input layer\n","# inputs = Input(shape=(256, 256, 3))\n","\n","# # First Convolutional Block\n","# x = Conv2D(16, (3,3), 1, activation='relu', padding='same')(inputs)\n","# x = MaxPooling2D()(x)\n","\n","# # Second Convolutional Block\n","# x = Conv2D(32, (3,3), 1, activation='relu', padding='same')(x)\n","# x = MaxPooling2D()(x)\n","\n","# # Third Convolutional Block\n","# x = Conv2D(16, (3,3), 1, activation='relu', padding='same')(x)\n","# x = MaxPooling2D()(x)\n","\n","# # Flatten layer\n","# x = Flatten()(x)\n","\n","# # Dense layers\n","# x = Dense(256, activation='relu')(x)\n","# # x = Dense(64, activation='relu')(x)\n","# outputs = Dense(1, activation='sigmoid')(x)\n","\n","# # Create the model\n","# model = Model(inputs=inputs, outputs=outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.116045,"end_time":"2024-05-15T05:45:57.891002","exception":false,"start_time":"2024-05-15T05:45:57.774957","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["learning_rate = 0.00003 \n","optimizer = Adam(learning_rate=learning_rate)\n","model.compile(optimizer = optimizer, loss='binary_crossentropy', metrics = ['accuracy'])\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.095997,"end_time":"2024-05-15T05:45:58.293959","exception":false,"start_time":"2024-05-15T05:45:58.197962","status":"completed"},"tags":[]},"source":["<h1> 4.  Training Convolutional Neural Network </h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["early_stopping_callback = EarlyStopping(monitor='val_loss', mode='min', patience=5, restore_best_weights=True)\n","bst_model = model.fit(training_data, epochs=5, validation_data=validation_data, callbacks=[early_stopping_callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.103221,"end_time":"2024-05-15T05:45:58.490141","exception":false,"start_time":"2024-05-15T05:45:58.386920","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# setting up for logging \n","logdir='logs'\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n","bst_model = model.fit(training_data, epochs=6, validation_data=validation_data, callbacks=[tensorboard_callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":65.339925,"end_time":"2024-05-15T05:47:03.924388","exception":false,"start_time":"2024-05-15T05:45:58.584463","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["model.save(\"/kaggle/working/accidents.keras\")                                                     "]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.135552,"end_time":"2024-05-15T05:47:04.194050","exception":false,"start_time":"2024-05-15T05:47:04.058498","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["bst_model.history\n","bst_model.history['val_accuracy'][-1]"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.129722,"end_time":"2024-05-15T05:47:04.449212","exception":false,"start_time":"2024-05-15T05:47:04.319490","status":"completed"},"tags":[]},"source":["<h2>5. Plotting Training Loss and Accuracy Curve with epochs</h2>"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.439505,"end_time":"2024-05-15T05:47:05.028922","exception":false,"start_time":"2024-05-15T05:47:04.589417","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["fig = plt.figure()\n","plt.plot(bst_model.history['loss'], color='red', label='training loss')\n","plt.plot(bst_model.history['val_loss'], color='blue', label='validation_loss')\n","fig.suptitle('Loss', fontsize=20)\n","plt.legend(loc=\"upper left\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"loss\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.406352,"end_time":"2024-05-15T05:47:05.561621","exception":false,"start_time":"2024-05-15T05:47:05.155269","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["fig = plt.figure()\n","plt.plot(bst_model.history['accuracy'], color='red', label='training accuracy')\n","plt.plot(bst_model.history['val_accuracy'], color='blue', label='validation_accuracy')\n","fig.suptitle('Accuracy', fontsize=20)\n","plt.legend(loc=\"upper left\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.127763,"end_time":"2024-05-15T05:47:05.825994","exception":false,"start_time":"2024-05-15T05:47:05.698231","status":"completed"},"tags":[]},"source":["# 6. Evaluation\n","Model performance will be measured by its F1 scores in predicting and analyzing actual traffic accidents.\n","Solutions offering actionable insights and demonstrable potential to reduce accident frequency and impact will be favoured."]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.591539,"end_time":"2024-05-15T05:47:06.545518","exception":false,"start_time":"2024-05-15T05:47:05.953979","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["test_data_dir = os.path.join(\"/kaggle/input/accident-detection-from-cctv-footage/data/test\")\n","test_data = tf.keras.utils.image_dataset_from_directory(test_data_dir)\n","test_data_iterator = test_data.as_numpy_iterator()\n","test_batch = test_data_iterator.next()"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":2.006276,"end_time":"2024-05-15T05:47:08.944765","exception":false,"start_time":"2024-05-15T05:47:06.938489","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import numpy as np\n","warnings.filterwarnings(\"ignore\")\n","\n","\n","y_true = []  # true labels\n","y_pred = []  # predicted labels\n","\n","for batch in test_data:\n","    X, y = batch\n","    yhat = model.predict(X)\n","    y_pred.extend(yhat.flatten().round().astype(int))\n","    y_true.extend(np.array(y).flatten().astype(int))  # Convert y to NumPy array\n","\n","# Calculate evaluation metrics\n","f1 = f1_score(y_true, y_pred)\n","precision = precision_score(y_true, y_pred)\n","recall = recall_score(y_true, y_pred)\n","cm = confusion_matrix(y_true, y_pred)\n","\n","# Print evaluation metrics and confusion matrix\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1 score:\", f1)\n","print(\"Confusion Matrix:\")\n","print(cm)\n","\n","# Generate classification report\n","target_names = ['Not Accident', 'Accident']\n","classification_rep = classification_report(y_true, y_pred, target_names=target_names)\n","print(\"Classification Report:\")\n","print(classification_rep)\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.129295,"end_time":"2024-05-15T05:47:10.062212","exception":false,"start_time":"2024-05-15T05:47:09.932917","status":"completed"},"tags":[]},"source":["# 7. Sanity check on Test"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import cv2\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","\n","# Define the directories for each class\n","class_directories = [\"/kaggle/input/accident-detection-from-cctv-footage/data/test/Accident\",\n","                     \"/kaggle/input/accident-detection-from-cctv-footage/data/test/Non Accident\"]\n","\n","# Randomly select one image from each class directory\n","selected_images = []\n","for directory in class_directories:\n","    filenames = os.listdir(directory)\n","    selected_image = random.choice(filenames)\n","    selected_image_path = os.path.join(directory, selected_image)\n","    selected_images.append(selected_image_path)\n","\n","# Load and resize the selected images\n","samples = []\n","for image_path in selected_images:\n","    sample = cv2.imread(image_path, cv2.IMREAD_COLOR)\n","    sample = cv2.resize(sample, (256, 256))\n","    samples.append(sample)\n","\n","# Perform prediction for each sample\n","t_label = \"Accident\"\n","for sample in samples:\n","    prediction = 1 - model.predict(np.expand_dims(sample / 255, 0))\n","\n","    if prediction >= 0.5:\n","        label = f'Predicted class is Accident; Actual is {t_label}'\n","    else:\n","        label = f'Predicted class is Not Accident; Actual is {t_label}'\n","\n","    plt.title(label)\n","    plt.imshow(sample)\n","    plt.show()\n","    \n","    t_label = \"Non Accident\"\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.136074,"end_time":"2024-05-15T05:47:11.903298","exception":false,"start_time":"2024-05-15T05:47:11.767224","status":"completed"},"tags":[]},"source":["### Create CSV Files for Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import cv2\n","import os\n","import pandas as pd\n","\n","# Define the directory containing the test data\n","test_data_dir = \"/kaggle/input/accident-detection-from-cctv-footage/data/test\"\n","\n","# Initialize lists to store filenames and predictions\n","filenames = []\n","labels=[]\n","predictions = []\n","\n","# Iterate through subdirectories in the test data directory\n","for subdir in os.listdir(test_data_dir):\n","    subdir_path = os.path.join(test_data_dir, subdir)\n","    c=1\n","    # Check if the item in the directory is a subdirectory\n","    if os.path.isdir(subdir_path):\n","        # Iterate through files in the subdirectory\n","        for filename in os.listdir(subdir_path):\n","            # Check if the file is a JPEG image\n","            if filename.endswith(\".jpg\"):\n","                filepath = os.path.join(subdir_path, filename)\n","                \n","                # Load and resize the image\n","                sample = cv2.imread(filepath, cv2.IMREAD_COLOR)\n","                sample = cv2.resize(sample, (256, 256))\n","                \n","                # Predict using the model\n","                prediction = model.predict(np.expand_dims(sample / 255, 0))\n","                \n","                # Assign labels based on the prediction\n","                output = 1 if prediction >= 0.5 else 0\n","                \n","                # Append filename and prediction to lists\n","                filenames.append(filename)\n","                predictions.append(output)\n","                labels.append(c)\n","    c=0\n","# Create a DataFrame to store filenames and predictions\n","df = pd.DataFrame({\"ID\": filenames, \"Column ID\": predictions})\n","\n","# Save the DataFrame to a CSV file\n","output_csv_path = \"/kaggle/working/submission.csv\"\n","df.to_csv(output_csv_path, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n","\n","# Calculate evaluation metrics\n","f1 = f1_score(y_true, y_pred)\n","precision = precision_score(y_true, y_pred)\n","recall = recall_score(y_true, y_pred)\n","cm = confusion_matrix(y_true, y_pred)\n","\n","# Print evaluation metrics and confusion matrix\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1 score:\", f1)\n","print(\"Confusion Matrix:\")\n","print(cm)\n","\n","# Generate classification report\n","target_names = ['Not Accident', 'Accident']\n","classification_rep = classification_report(y_true, y_pred, target_names=target_names)\n","print(\"Classification Report:\")\n","print(classification_rep)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Future work\n","\n","Captions generation through image-to-text model which describes the imgae in details using pre-trained models like \n","gpt2 (https://huggingface.co/nlpconnect/vit-gpt2-image-captioning) or \n","blip (https://huggingface.co/Salesforce/blip-image-captioning-large)\n","etc.\n","\n",">> "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModel\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n","img_to_text_model = AutoModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n","\n","# Freeze the model\n","img_to_text_model.trainable = False"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":8428710,"sourceId":77305,"sourceType":"competition"},{"datasetId":804753,"sourceId":1379553,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":116.215069,"end_time":"2024-05-15T05:47:23.279419","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-15T05:45:27.064350","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}
