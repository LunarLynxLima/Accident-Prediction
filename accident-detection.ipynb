{"cells":[{"cell_type":"markdown","metadata":{"papermill":{"duration":0.012607,"end_time":"2024-05-15T05:45:29.670720","exception":false,"start_time":"2024-05-15T05:45:29.658113","status":"completed"},"tags":[]},"source":["<h1> Accident Detection From CCTV Footage </h1>"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.011721,"end_time":"2024-05-15T05:45:29.694146","exception":false,"start_time":"2024-05-15T05:45:29.682425","status":"completed"},"tags":[]},"source":["<h2>Description :</h2>\n","<h3>Dataset Description :</h3>\n","<p> Accident Detection dataset collected from the CCTV footages containing a total of 990 accident and non-accident frames collected from road videos available on YouTube. The 990 files are split in the 791 training frames, 101 test frames and 98 validation frames.\n","791 (369-accident, 492-non accident) Training, 101 Test and 98 Validation (52-accident, 46-non accident) frames split in Accident and Non-accident frames in all the three folders. </p>\n","\n","<h3>Problem Analysis: </h3>\n","<pre>\n","Input : Images that can be accident/Non Accident\n","Output : 0(Indicates No Accident)\n","         1(Indicates Accident)\n","</pre>"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.011443,"end_time":"2024-05-15T05:45:29.717104","exception":false,"start_time":"2024-05-15T05:45:29.705661","status":"completed"},"tags":[]},"source":["<h1>1. Loading Data</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:57:51.338846Z"},"papermill":{"duration":11.656721,"end_time":"2024-05-15T05:45:41.421953","exception":false,"start_time":"2024-05-15T05:45:29.765232","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":2.514304,"end_time":"2024-05-15T05:45:43.948147","exception":false,"start_time":"2024-05-15T05:45:41.433843","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["training_data_dir = os.path.join(\"/kaggle/input/accident-detection-from-cctv-footage/data/train\")\n","training_data = tf.keras.utils.image_dataset_from_directory(\n","                            training_data_dir,image_size=(256, 256),\n","                            seed = 42\n","                            )"]},{"cell_type":"code","execution_count":null,"id":"9a8bc5d1","metadata":{"papermill":{"duration":1.101441,"end_time":"2024-05-15T05:45:45.061537","exception":false,"start_time":"2024-05-15T05:45:43.960096","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["#  iter extracts in each batch of 32 images \n","training_data_iter = training_data.as_numpy_iter()\n","training_batch = training_data_iter.next()"]},{"cell_type":"markdown","id":"4a7b28fe","metadata":{"papermill":{"duration":0.049282,"end_time":"2024-05-15T05:45:49.779984","exception":false,"start_time":"2024-05-15T05:45:49.730702","status":"completed"},"tags":[]},"source":["<h1>2. Preprocessing Data </h1>"]},{"cell_type":"code","execution_count":null,"id":"4495585c","metadata":{"papermill":{"duration":0.081079,"end_time":"2024-05-15T05:45:49.911488","exception":false,"start_time":"2024-05-15T05:45:49.830409","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Normalizing rgb pixels value between between 0 & 1 \n","training_data = training_data.map(lambda x,y: (x/255, y))\n","training_batch = training_data.as_numpy_iter().next()\n","\n","# Sanity Check pixel min/max pixel values after normalization\n","print(\"Max pixel value : \",training_batch[0].max())\n","print(\"Min pixel value : \",training_batch[0].min())"]},{"cell_type":"markdown","id":"11fb50e7","metadata":{"papermill":{"duration":0.089998,"end_time":"2024-05-15T05:45:55.536259","exception":false,"start_time":"2024-05-15T05:45:55.446261","status":"completed"},"tags":[]},"source":["<h2>Loading Validation data for Hyper-parameter Turing</h2>"]},{"cell_type":"code","execution_count":null,"id":"0faba709","metadata":{"papermill":{"duration":0.515378,"end_time":"2024-05-15T05:45:56.137784","exception":false,"start_time":"2024-05-15T05:45:55.622406","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["validation_data_dir = os.path.join(\"/kaggle/input/accident-detection-from-cctv-footage/data/val\")\n","validation_data = tf.keras.utils.image_dataset_from_directory(validation_data_dir)\n","validation_data_iter = validation_data.as_numpy_iter()\n","validation_batch = validation_data_iter.next()"]},{"cell_type":"code","execution_count":null,"id":"c8614a28","metadata":{"papermill":{"duration":0.365998,"end_time":"2024-05-15T05:45:56.594839","exception":false,"start_time":"2024-05-15T05:45:56.228841","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Normalizing Validation data\n","validation_data = validation_data.map(lambda x,y: (x/255, y))\n","validation_batch = validation_data.as_numpy_iter().next()\n","\n","# Sanity Check pixel min/max pixel values after normalization\n","print(\"Max pixel value : \",validation_batch[0].max())\n","print(\"Min pixel value : \",validation_batch[0].min())"]},{"cell_type":"markdown","id":"70a03ea0","metadata":{"papermill":{"duration":0.085089,"end_time":"2024-05-15T05:45:56.948892","exception":false,"start_time":"2024-05-15T05:45:56.863803","status":"completed"},"tags":[]},"source":["<h1> 3. Building CNN Architecture  </h1>\n"]},{"cell_type":"code","execution_count":null,"id":"6ec1a0fa","metadata":{"papermill":{"duration":0.097538,"end_time":"2024-05-15T05:45:57.131534","exception":false,"start_time":"2024-05-15T05:45:57.033996","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Add, Dropout\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.optimizers import Adam"]},{"cell_type":"code","execution_count":null,"id":"66abf294","metadata":{"papermill":{"duration":0.100246,"end_time":"2024-05-15T05:45:57.358321","exception":false,"start_time":"2024-05-15T05:45:57.258075","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Define input layer\n","inputs = Input(shape=(256, 256, 3))\n","\n","# First Convolutional Block\n","x = Conv2D(16, (3,3), 1, activation='relu', padding='same')(inputs)\n","x = MaxPooling2D()(x)\n","\n","# Second Convolutional Block with residual connection\n","conv1 = Conv2D(32, (3,3), 1, activation='relu', padding='same')(x)\n","conv2 = Conv2D(32, (3,3), 1, activation='relu', padding='same')(conv1)\n","# Adding convolutional layer to match the number of channels\n","residual = Conv2D(32, (1, 1), strides=(1, 1), padding='same')(x)\n","residual = Add()([residual, conv2])\n","x = MaxPooling2D()(residual)\n","\n","# Third Convolutional Block with residual connection\n","conv3 = Conv2D(16, (3,3), 1, activation='relu', padding='same')(x)\n","conv4 = Conv2D(16, (3,3), 1, activation='relu', padding='same')(conv3)\n","# Adding convolutional layer to match the number of channels\n","residual = Conv2D(16, (1, 1), strides=(1, 1), padding='same')(x)\n","residual = Add()([residual, conv4])\n","x = MaxPooling2D()(residual)\n","\n","# Flatten layer\n","x = Flatten()(x)\n","\n","# Fully connected layers\n","x = Dense(256, activation='relu')(x)\n","outputs = Dense(1, activation='sigmoid')(x)"]},{"cell_type":"code","execution_count":null,"id":"55ad036e","metadata":{"papermill":{"duration":0.226306,"end_time":"2024-05-15T05:45:57.672649","exception":false,"start_time":"2024-05-15T05:45:57.446343","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# model = Sequential()\n","\n","# model.add(Conv2D(16, (3,3), 1, activation='relu', input_shape=(256,256,3)))\n","# model.add(MaxPooling2D())\n","# model.add(Conv2D(32, (3,3), 1, activation='relu'))\n","# model.add(MaxPooling2D())\n","# model.add(Conv2D(16, (3,3), 1, activation='relu'))\n","# model.add(MaxPooling2D())\n","# model.add(Flatten())\n","# # Adding neural Layer\n","# model.add(Dense(256, activation='relu'))\n","# model.add(Dense(1, activation='sigmoid'))"]},{"cell_type":"code","execution_count":null,"id":"f825f06d","metadata":{"papermill":{"duration":0.116045,"end_time":"2024-05-15T05:45:57.891002","exception":false,"start_time":"2024-05-15T05:45:57.774957","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["learning_rate = 0.001 \n","optimizer = Adam(learning_rate=learning_rate)\n","model = Model(inputs=inputs, outputs=outputs)\n","model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"id":"3eb8274e","metadata":{"papermill":{"duration":0.111979,"end_time":"2024-05-15T05:45:58.099358","exception":false,"start_time":"2024-05-15T05:45:57.987379","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","id":"a309f66c","metadata":{"papermill":{"duration":0.095997,"end_time":"2024-05-15T05:45:58.293959","exception":false,"start_time":"2024-05-15T05:45:58.197962","status":"completed"},"tags":[]},"source":["<h1> 4.  Training Neural Network </h1>"]},{"cell_type":"code","execution_count":null,"id":"6cbd58c2","metadata":{"papermill":{"duration":0.103221,"end_time":"2024-05-15T05:45:58.490141","exception":false,"start_time":"2024-05-15T05:45:58.386920","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# setting up for logging \n","logdir='logs'\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n","early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"]},{"cell_type":"code","execution_count":null,"id":"df9c7989","metadata":{"papermill":{"duration":65.339925,"end_time":"2024-05-15T05:47:03.924388","exception":false,"start_time":"2024-05-15T05:45:58.584463","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["bst_model = model.fit(training_data, epochs=50, validation_data=validation_data, callbacks=[early_stopping_callback])\n","bst_model = model.fit(training_data, epochs=50, validation_data=validation_data, callbacks=[tensorboard_callback])\n","model.save(\"/kaggle/working/accidents.keras\")                                                     "]},{"cell_type":"code","execution_count":null,"id":"556b6297","metadata":{"papermill":{"duration":0.135552,"end_time":"2024-05-15T05:47:04.194050","exception":false,"start_time":"2024-05-15T05:47:04.058498","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["bst_model.history['validation_accuracy'][-1]"]},{"cell_type":"markdown","id":"1563766f","metadata":{"papermill":{"duration":0.129722,"end_time":"2024-05-15T05:47:04.449212","exception":false,"start_time":"2024-05-15T05:47:04.319490","status":"completed"},"tags":[]},"source":["<h2>5.Seeing Training Loss and Accuracy Curve with epochs</h2>"]},{"cell_type":"code","execution_count":null,"id":"941e8550","metadata":{"papermill":{"duration":0.439505,"end_time":"2024-05-15T05:47:05.028922","exception":false,"start_time":"2024-05-15T05:47:04.589417","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["fig = plt.figure()\n","plt.plot(bst_model.history['loss'], color='red', label='training loss')\n","plt.plot(bst_model.history['validation_loss'], color='blue', label='validation_loss')\n","fig.suptitle('Loss', fontsize=20)\n","plt.legend(loc=\"upper left\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"loss\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"5834fd80","metadata":{"papermill":{"duration":0.406352,"end_time":"2024-05-15T05:47:05.561621","exception":false,"start_time":"2024-05-15T05:47:05.155269","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["fig = plt.figure()\n","plt.plot(bst_model.history['accuracy'], color='red', label='training accuracy')\n","plt.plot(bst_model.history['validation_accuracy'], color='blue', label='validation_accuracy')\n","fig.suptitle('Accuracy', fontsize=20)\n","plt.legend(loc=\"upper left\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.show()"]},{"cell_type":"markdown","id":"4465f085","metadata":{"papermill":{"duration":0.127763,"end_time":"2024-05-15T05:47:05.825994","exception":false,"start_time":"2024-05-15T05:47:05.698231","status":"completed"},"tags":[]},"source":["<h1>6. Evaluation</h1>"]},{"cell_type":"code","execution_count":null,"id":"34ddb8fb","metadata":{"papermill":{"duration":0.591539,"end_time":"2024-05-15T05:47:06.545518","exception":false,"start_time":"2024-05-15T05:47:05.953979","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["test_data_dir = os.path.join(\"/kaggle/input/accident-detection-from-cctv-footage/data/test\")\n","test_data = tf.keras.utils.image_dataset_from_directory(test_data_dir)\n","test_data_iter = test_data.as_numpy_iter()\n","test_batch = test_data_iter.next()"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.139674,"end_time":"2024-05-15T05:47:06.812049","exception":false,"start_time":"2024-05-15T05:47:06.672375","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["pre = tf.keras.metrics.Precision\n","re = tf.keras.metrics.Recall()"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":2.006276,"end_time":"2024-05-15T05:47:08.944765","exception":false,"start_time":"2024-05-15T05:47:06.938489","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["pre = tf.keras.metrics.Precision()\n","re = tf.keras.metrics.Recall()\n","\n","for batch in test_data:\n","    X, y = batch\n","    yhat = model.predict(X)\n","    pre.update_state(y, yhat)\n","    re.update_state(y, yhat)"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.154956,"end_time":"2024-05-15T05:47:09.241290","exception":false,"start_time":"2024-05-15T05:47:09.086334","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def F1_score(precision, recall):\n","    return (2*precision*recall)/(precision+recall)"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.15238,"end_time":"2024-05-15T05:47:09.528727","exception":false,"start_time":"2024-05-15T05:47:09.376347","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["print(\"Model achieved an precision score of {:5f}\".format(pre.result()))\n","print(\"Model achieved an recall score of {:5f}\".format(re.result()))"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.144909,"end_time":"2024-05-15T05:47:09.803012","exception":false,"start_time":"2024-05-15T05:47:09.658103","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["f1_score = F1_score(pre.result(), re.result())\n","print(\"Model achieved an F1-score of {:5f}\".format(f1_score))"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.129295,"end_time":"2024-05-15T05:47:10.062212","exception":false,"start_time":"2024-05-15T05:47:09.932917","status":"completed"},"tags":[]},"source":["<h1> 7.Test just to see model working </h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":1.434368,"end_time":"2024-05-15T05:47:11.627265","exception":false,"start_time":"2024-05-15T05:47:10.192897","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import cv2\n","\n","# load random samples from samples directory\n","random_data_dirname = os.path.join(\"/kaggle/input/accident-detection-from-cctv-footage/data/test/Accident\")\n","pics = [os.path.join(random_data_dirname, filename) for filename in os.listdir(random_data_dirname)]\n","\n","# load first file from samples\n","sample = cv2.imread(pics[1], cv2.IMREAD_COLOR)\n","sample = cv2.resize(sample, (256, 256))\n","\n","prediction = 1 - model.predict(np.expand_dims(sample/255, 0))\n","\n","if prediction >= 0.5: \n","    label = 'Predicted class is Accident'\n","else:\n","    label = 'Predicted class is Not Accident'\n","\n","plt.title(label)\n","plt.imshow(sample)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.136074,"end_time":"2024-05-15T05:47:11.903298","exception":false,"start_time":"2024-05-15T05:47:11.767224","status":"completed"},"tags":[]},"source":["### Create CSV Files for Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":7.355022,"end_time":"2024-05-15T05:47:19.391935","exception":false,"start_time":"2024-05-15T05:47:12.036913","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import cv2\n","import pandas as pd\n","\n","# load random samples from samples directory\n","test_data_dirname = os.path.join(\"/kaggle/input/accident-detection-from-cctv-footage/data/test\")\n","pics = [os.path.join(test_data_dirname, filename) for filename in os.listdir(test_data_dirname)]\n","\n","\n","filenames = []\n","predictions = []\n","\n","for dirname in os.listdir(test_data_dirname):\n","    for filename in os.listdir(os.path.join(test_data_dirname, dirname)):\n","        if not filename.endswith(\".jpg\"):\n","            continue\n","        filepath = os.path.join(test_data_dirname, dirname, filename)\n","        \n","        # load first file from samples\n","        sample = cv2.imread(filepath, cv2.IMREAD_COLOR)\n","        sample = cv2.resize(sample, (256, 256))\n","        \n","        # predict using model\n","        prediction = 1 - model.predict(np.expand_dims(sample/255, 0))\n","        # done because when we loaded data by default 0 label is given to first folder\n","        # which is Accident but we want just opposite labels\n","        # we want 0: Accident and 1: Not Accident\n","        \n","        filenames.append(filename)\n","        \n","        output = 1 if float(prediction[0][0]) >= 0.5 else 0\n","        predictions.append(output)\n","\n","df = pd.DataFrame(columns=[\"ID\", \"Column ID\"])\n","df[\"ID\"] = filenames\n","df[\"Column ID\"] = predictions\n","df.to_csv(\"/kaggle/working/submission.csv\",index=False)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8428710,"sourceId":77305,"sourceType":"competition"},{"datasetId":804753,"sourceId":1379553,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":116.215069,"end_time":"2024-05-15T05:47:23.279419","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-15T05:45:27.064350","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}
